# üöÄ API de IA Optimizada - Carga bajo demanda
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Literal, Optional
import uvicorn
import logging

# Imports de LangChain y HuggingFace (solo cuando sea necesario)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# üìã Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# üß† Variables globales para los modelos (inicialmente None)
translator_llm = None
chains = {}

# üìä Modelos Pydantic
class TranslationRequest(BaseModel):
    texto: str = Field(..., min_length=1, max_length=500, description="Texto en ingl√©s a traducir")
    estilo: Literal["basico", "formal", "creativo"] = Field(default="basico", description="Estilo de traducci√≥n")

class TranslationResponse(BaseModel):
    texto_original: str
    texto_traducido: str
    estilo_usado: str

# üîß Funci√≥n para cargar traductor bajo demanda
def load_translator():
    """Carga el modelo de traducci√≥n solo cuando se necesita"""
    global translator_llm, chains
    
    if translator_llm is not None:
        return  # Ya est√° cargado
    
    try:
        logger.info("ü§ñ Cargando modelo de traducci√≥n bajo demanda...")
        
        from langchain_huggingface import HuggingFacePipeline
        from transformers import pipeline
        
        translator_pipeline = pipeline(
            "translation",
            model="Helsinki-NLP/opus-mt-en-es",
            device="cpu"
        )
        translator_llm = HuggingFacePipeline(pipeline=translator_pipeline)
        
        # Crear prompts
        prompt_basico = PromptTemplate(
            input_variables=["texto"],
            template="{texto}"
        )
        
        prompt_formal = PromptTemplate(
            input_variables=["texto"], 
            template="Traduce el siguiente texto al espa√±ol de manera como si fuera para ni√±os de 5 a√±os: {texto}"
        )
        
        prompt_creativo = PromptTemplate(
            input_variables=["texto"],
            template="Traduce el siguiente texto al espa√±ol de manera para un animal: {texto}"
        )
        
        # Crear chains
        output_parser = StrOutputParser()
        chains = {
            "basico": prompt_basico | translator_llm | output_parser,
            "formal": prompt_formal | translator_llm | output_parser,
            "creativo": prompt_creativo | translator_llm | output_parser
        }
        
        logger.info("‚úÖ Modelo de traducci√≥n cargado exitosamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error cargando modelo de traducci√≥n: {str(e)}")
        raise

# üöÄ Crear la aplicaci√≥n FastAPI
app = FastAPI(
    title="ü§ñ API de IA Optimizada",
    description="API con carga bajo demanda para traducci√≥n ingl√©s-espa√±ol",
    version="1.0.0"
)

# üè† Endpoint ra√≠z
@app.get("/")
async def root():
    return {
        "message": "ü§ñ API de IA Optimizada activa",
        "status": "ready",
        "endpoints": {
            "traduccion": "/translate",
            "salud": "/health",
            "documentacion": "/docs"
        }
    }

# ‚ù§Ô∏è Endpoint de health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "traductor_cargado": translator_llm is not None,
        "memoria_usage": "optimizado - carga bajo demanda"
    }

# üî§ Endpoint de traducci√≥n (con carga bajo demanda)
@app.post("/translate", response_model=TranslationResponse)
async def translate_text(request: TranslationRequest):
    """
    Traduce texto de ingl√©s a espa√±ol con diferentes estilos
    El modelo se carga autom√°ticamente en la primera petici√≥n
    """
    try:
        # Cargar modelo solo si es necesario
        if translator_llm is None:
            logger.info("‚è≥ Primera petici√≥n - cargando modelo...")
            load_translator()
        
        if request.estilo not in chains:
            raise HTTPException(status_code=400, detail=f"Estilo '{request.estilo}' no v√°lido")
        
        # Ejecutar traducci√≥n
        traduccion = chains[request.estilo].invoke({"texto": request.texto})
        
        return TranslationResponse(
            texto_original=request.texto,
            texto_traducido=traduccion,
            estilo_usado=request.estilo
        )
        
    except Exception as e:
        logger.error(f"Error en traducci√≥n: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error en traducci√≥n: {str(e)}")

# üß™ Endpoint de demo r√°pido (sin IA)
@app.get("/demo")
async def demo():
    """Demo que funciona sin cargar modelos"""
    return {
        "üéØ": "API Demo",
        "status": "funcionando",
        "ejemplo_traduccion": "Usa POST /translate para traducir",
        "modelo_estado": "carga bajo demanda" if translator_llm is None else "cargado"
    }

# üèÉ‚Äç‚ôÇÔ∏è Ejecutar la aplicaci√≥n
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="127.0.0.1", 
        port=8000,  # Puerto est√°ndar
        reload=True
    ) 